
# BERT
## 내용없음 <br/><br/>


## 학습 데이터 
## [한국어 단발성 대화](https://aihub.or.kr/opendata/keti-data/recognition-laguage/KETI-02-009)<br/><br/>


## 파라미터  
1. batch_size = 64
2. MAX_LEN = 64
3. num_epochs = 5
4. Learning rage(Adam) = 2e-5
5. epsilon(Adam) = 1e-8
6. seed_val = 42  <br/><br/>



## 학습 결과
## 학습 데이터 정확도 : 0.5 / 테스트 데이터 정확도 0.5


#### [참고](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)
