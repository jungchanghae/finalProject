# -*- coding: utf-8 -*-
"""XLM-Roberta.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-8loz0wEKhA7xOX7KoqXXN1NWzqanjDW
"""

!pip install mxnet
!pip install gluonnlp
!pip install transformers
!pip install sentencepiece

!pip install simpletransformers

import transformers

import os 
import random 
import time 
import torch 

import pandas as pd 
import numpy as np 
from transformers import BertTokenizer
from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, BertConfig 
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import accuracy_score
from keras.preprocessing.sequence import pad_sequences

import datetime

import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
n_gpu = torch.cuda.device_count()
torch.cuda.get_device_name(0)

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_excel('/content/drive/MyDrive/PIG/한국어_단발성_대화_데이터셋.xlsx')
df.head()

df.info()

df_shuffled = df.sample(frac=1).reset_index(drop=True)
df_shuffled.head()

print(df_shuffled.shape)

df_shuffled.loc[(df_shuffled['Emotion'] == "공포"), 'Emotion'] = 0  #공포 => 0
df_shuffled.loc[(df_shuffled['Emotion'] == "놀람"), 'Emotion'] = 1  #놀람 => 1
df_shuffled.loc[(df_shuffled['Emotion'] == "분노"), 'Emotion'] = 2  #분노 => 2
df_shuffled.loc[(df_shuffled['Emotion'] == "슬픔"), 'Emotion'] = 3  #슬픔 => 3
df_shuffled.loc[(df_shuffled['Emotion'] == "중립"), 'Emotion'] = 4  #중립 => 4
df_shuffled.loc[(df_shuffled['Emotion'] == "행복"), 'Emotion'] = 5  #행복 => 5
df_shuffled.loc[(df_shuffled['Emotion'] == "혐오"), 'Emotion'] = 6  #혐오 => 6

dataset_train, dataset_test = train_test_split(df_shuffled, test_size=0.25, random_state=0)

print(len(dataset_train))
print(len(dataset_test))

dataset_train.head()

from transformers import AutoTokenizer, AdamW, RobertaForSequenceClassification
from torch.utils.data import DataLoader, Dataset

# XLM-RoBERTa 토크나이저를 불러옵니다: https://huggingface.co/xlm-roberta-large
from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification

MODEL_TYPE = 'xlm-roberta-large'
# 만약 colab pro가 아니면 MODEL_TYPE = 'xlm-roberta-base'를 사용하세요
tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)

from transformers import AutoTokenizer, AdamW, RobertaForSequenceClassification
from torch.utils.data import DataLoader, Dataset

tokenizer.vocab_size

list(tokenizer.get_vocab())[:10]

device= torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

print(device)

row = df_shuffled.iloc[0, 0:2].values
text = row[0]
y = row[1]
print(row)
print(text)
print(y)

class CompDataset(Dataset):

    def __init__(self, csv_file):
        self.dataset = csv_file
        self.tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')

        print(self.dataset.describe())

    def __getitem__(self, idx):

        row = self.dataset.iloc[idx, 0:2].values
        text = row[0]
        y = row[1]

        encoded_dict = tokenizer.encode_plus(
                    text,      
                    add_special_tokens = True,      
                    max_length = MAX_LEN,           
                    pad_to_max_length = True,
                    truncation=True,
                    return_attention_mask = True,   
                    return_tensors = 'pt',          
               )
        
        input_ids = encoded_dict['input_ids'][0]
        attention_mask = encoded_dict['attention_mask'][0]

        return input_ids, attention_mask, y

    def __len__(self):
        return len(self.dataset)
    
# test 예측에 사용
class TestDataset(Dataset):

    def __init__(self, csv_file):
        self.dataset = csv_file
        self.tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')
    def __getitem__(self, idx):

        row = self.dataset.iloc[idx, 0:1].values
        text = row[0]



        encoded_dict = tokenizer.encode_plus(
                    text,     
                    add_special_tokens = True,      
                    max_length = MAX_LEN,           
                    pad_to_max_length = True,
                    return_attention_mask = True,   
                    truncation=True,
                    return_tensors = 'pt',          
               )
        
        input_ids = encoded_dict['input_ids'][0]
        attention_mask = encoded_dict['attention_mask'][0]
        # input_ids, attention_mask를 하나의 인풋으로 묶음
     

        return input_ids, attention_mask


    def __len__(self):
        return len(self.dataset)

L_RATE = 1e-5 
MAX_LEN = 32 

BATCH_SIZE = 32 # batch size가 클수록 global minimum에 도달하는 속도가 증가합니다. (GPU 메모리에 따라 변경해 주세요, 너무 크면 OOM 문제가 발생합니다.)
NUM_CORES = os.cpu_count() # Dataloader에 사용됩니다. 

NUM_CORES

from transformers import XLMRobertaForSequenceClassification


import gc
gc.collect()
torch.cuda.empty_cache()
model = XLMRobertaForSequenceClassification.from_pretrained(
    MODEL_TYPE, 
    num_labels = 7, # 출력 label의 개수
)

# model을 device위에 올림
model.to(device)

train_data = CompDataset(dataset_train)
test_data = CompDataset(dataset_test)

# batch_size 만큼 데이터 분할
train_loader = DataLoader(train_data,
                                batch_size=BATCH_SIZE,
                                shuffle=True,
                                num_workers=0)


test_loader = DataLoader(test_data,
                                batch_size=BATCH_SIZE,
                                shuffle=True,
                                num_workers=0)



print(len(train_loader))

print(len(test_loader))

optimizer = AdamW(model.parameters(),
              lr = L_RATE, 
              eps = 1e-8 
            )

gc.collect()
torch.cuda.empty_cache()

from tqdm.notebook import tqdm
from torch.nn import functional as F

# train
losses = []
accuracies = []
total_loss = 0.0
correct = 0
total = 0
epochs = 5
for i in range(epochs):

    model.train()

    for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):
        optimizer.zero_grad()
        y_batch = y_batch.to(device)
        y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]
        loss = F.cross_entropy(y_pred, y_batch)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        _, predicted = torch.max(y_pred, 1)
        correct += (predicted == y_batch).sum()
        total += len(y_batch)

    losses.append(total_loss)
    accuracies.append(correct.float() / total)
    print("Train Loss:", total_loss / total, "Accuracy:", correct.float() / total)

# validation
model.eval()

pred = []
correct = 0
total = 0

for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):
  y_batch = y_batch.to(device)
  y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]
  _, predicted = torch.max(y_pred, 1)
  pred.append(predicted)
  correct += (predicted == y_batch).sum()
  total += len(y_batch)

print("val accuracy:", correct.float() / total)

def predict(predict_sentence):

    data = [predict_sentence]
    dataset_another = pd.DataFrame(data)

    another_test = TestDataset(dataset_another)
    test_dataloader = DataLoader(another_test, batch_size=BATCH_SIZE, num_workers=5)
    
    model.eval()

    for input_ids_batch, attention_masks_batch in tqdm(test_dataloader):
        y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]
        _, predicted = torch.max(y_pred, 1)
        pred.extend(predicted.tolist())


        test_eval=[]
        for i in y_pred:
            logits=i
            logits = logits.detach().cpu().numpy()

            if np.argmax(logits) == 0:
                test_eval.append("공포가")
            elif np.argmax(logits) == 1:
                test_eval.append("놀람이")
            elif np.argmax(logits) == 2:
                test_eval.append("분노가")
            elif np.argmax(logits) == 3:
                test_eval.append("슬픔이")
            elif np.argmax(logits) == 4:
                test_eval.append("중립이")
            elif np.argmax(logits) == 5:
                test_eval.append("행복이")
            elif np.argmax(logits) == 6:
                test_eval.append("혐오가")

        print(">> 입력하신 내용에서 " + test_eval[0] + " 느껴집니다.")

end = 1
while end == 1 :
    sentence = input("하고싶은 말을 입력해주세요 : ")
    if sentence == "0" :
        break
    predict(sentence)
    print("\n")

